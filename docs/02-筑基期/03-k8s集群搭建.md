# k8s集群搭建

使用虚拟机配置好单节点k8s环境后，将当前环境重置，然后复制虚拟机，创建3个节点的集群。

执行如下命令清除已部署的集群：

```shell
kubectl delete --all pods --all-namespaces
kubectl delete --all services --all-namespaces
kubectl delete --all deployments --all-namespaces
kubectl delete --all replicaSets --all-namespaces
kubectl delete --all statefulSets --all-namespaces
kubectl delete --all daemonSets --all-namespaces
kubectl delete --all configmaps --all-namespaces
kubectl delete --all secrets --all-namespaces
kubectl delete --all ingress --all-namespaces

```

重置 Kubernetes 集群

```shell
sudo kubeadm reset --cri-socket=unix:///var/run/cri-dockerd.sock
```

## 配置其他节点

克隆出来三台机器后，分别设置机器的hostname为node1、node2、node3.

每个节点的/etc/hosts设置成如下内容：

```shell
192.168.3.96 node1
192.168.3.95 node2
192.168.3.94 node3
```

确认一下是否能ping通：


```shell
root@node1:/home/dys# ping node2
PING node2 (192.168.3.95) 56(84) bytes of data.
64 bytes from node2 (192.168.3.95): icmp_seq=1 ttl=64 time=0.359 ms
64 bytes from node2 (192.168.3.95): icmp_seq=2 ttl=64 time=0.372 ms
64 bytes from node2 (192.168.3.95): icmp_seq=3 ttl=64 time=0.333 ms
^C
--- node2 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2029ms
rtt min/avg/max/mdev = 0.333/0.354/0.372/0.016 ms
root@node1:/home/dys# ping node
ping: node: Temporary failure in name resolution
root@node1:/home/dys# ping node3
PING node3 (192.168.3.94) 56(84) bytes of data.
64 bytes from node3 (192.168.3.94): icmp_seq=1 ttl=64 time=0.636 ms
64 bytes from node3 (192.168.3.94): icmp_seq=2 ttl=64 time=0.376 ms
64 bytes from node3 (192.168.3.94): icmp_seq=3 ttl=64 time=0.416 ms
^C
--- node3 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2035ms
rtt min/avg/max/mdev = 0.376/0.476/0.636/0.114 ms
```

## 初始化node1节点

在node1节点执行如下命令，初始化k8s集群

```shell
kubeadm init --apiserver-advertise-address=192.168.3.96 --pod-network-cidr=10.244.0.0/16 --image-repository=registry.aliyuncs.com/google_containers  --ignore-preflight-errors=Swap --cri-socket=unix:///var/run/cri-dockerd.sock -v=5
```

执行成功后会输出如下内容，需要记住该命令，后续还要使用：

```shell
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.3.96:6443 --token yb65a1.p045bn1algxpvl5h \
        --discovery-token-ca-cert-hash sha256:fc7bc23017475064f73df1f2d91b30a80e4a405e77519a53524109d211eb7132
```

初始化完成后，配置 kubectl 以便本地使用 Kubernetes API：

```shell
mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

然后执行如下命令查看一下当前节点：

```shell
root@node1:/home/dys# kubectl get nodes
NAME    STATUS   ROLES           AGE     VERSION
node1   Ready    control-plane   3m48s   v1.28.2
```

执行如下命令查看当前的资源情况：

```shell
root@node1:/home/dys# kubectl get all --all-namespaces
NAMESPACE     NAME                                READY   STATUS              RESTARTS   AGE
kube-system   pod/coredns-66f779496c-gkdp7        0/1     ContainerCreating   0          5m50s
kube-system   pod/coredns-66f779496c-rjtgp        0/1     ContainerCreating   0          5m50s
kube-system   pod/etcd-node1                      1/1     Running             0          6m4s
kube-system   pod/kube-apiserver-node1            1/1     Running             0          6m4s
kube-system   pod/kube-controller-manager-node1   1/1     Running             0          6m4s
kube-system   pod/kube-proxy-pvw92                1/1     Running             0          5m51s
kube-system   pod/kube-scheduler-node1            1/1     Running             0          6m4s

NAMESPACE     NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
default       service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP                  6m7s
kube-system   service/kube-dns     ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   6m5s

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-system   daemonset.apps/kube-proxy   1         1         1       1            1           kubernetes.io/os=linux   6m5s

NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   deployment.apps/coredns   0/2     2            0           6m5s

NAMESPACE     NAME                                 DESIRED   CURRENT   READY   AGE
kube-system   replicaset.apps/coredns-66f779496c   2         2         0       5m51s
```

确认一下是否安装了网络插件：

```shell
root@node1:/home/dys# kubectl get pods -n kube-system
NAME                            READY   STATUS              RESTARTS   AGE
coredns-66f779496c-gkdp7        0/1     ContainerCreating   0          6m56s
coredns-66f779496c-rjtgp        0/1     ContainerCreating   0          6m56s
etcd-node1                      1/1     Running             0          7m10s
kube-apiserver-node1            1/1     Running             0          7m10s
kube-controller-manager-node1   1/1     Running             0          7m10s
kube-proxy-pvw92                1/1     Running             0          6m57s
kube-scheduler-node1            1/1     Running             0          7m10s
```

没有安装的话使用如下命令进行安装，安装完成后再检查一下：

```shell
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

```shell
root@node1:/home/dys# kubectl get pods -n kube-system
NAME                            READY   STATUS    RESTARTS   AGE
coredns-66f779496c-gkdp7        1/1     Running   0          11m
coredns-66f779496c-rjtgp        1/1     Running   0          11m
etcd-node1                      1/1     Running   0          11m
kube-apiserver-node1            1/1     Running   0          11m
kube-controller-manager-node1   1/1     Running   0          11m
kube-proxy-pvw92                1/1     Running   0          11m
kube-scheduler-node1            1/1     Running   0          11m
root@node1:/home/dys# kubectl get pods -n kube-flannel
NAME                    READY   STATUS    RESTARTS   AGE
kube-flannel-ds-p4sqj   1/1     Running   0          2m32s
```

## 配置node2

执行如下命令加入集群：

```shell
kubeadm join 192.168.3.96:6443 --token yb65a1.p045bn1algxpvl5h \
        --discovery-token-ca-cert-hash sha256:fc7bc23017475064f73df1f2d91b30a80e4a405e77519a53524109d211eb7132 --cri-socket=unix:///var/run/cri-dockerd.sock
```

加入完成后可能会报验证错误，需要从node1从小拷贝一下认证信息：

```shell
root@node3:/home/dys# kubectl get nodes
E0414 02:22:46.133709   44601 memcache.go:265] couldn't get current server API group list: Get "https://192.168.3.96:6443/api?timeout=32s": tls: failed to verify certificate: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "kubernetes")
E0414 02:22:46.137275   44601 memcache.go:265] couldn't get current server API group list: Get "https://192.168.3.96:6443/api?timeout=32s": tls: failed to verify certificate: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "kubernetes")
E0414 02:22:46.140631   44601 memcache.go:265] couldn't get current server API group list: Get "https://192.168.3.96:6443/api?timeout=32s": tls: failed to verify certificate: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "kubernetes")
E0414 02:22:46.143908   44601 memcache.go:265] couldn't get current server API group list: Get "https://192.168.3.96:6443/api?timeout=32s": tls: failed to verify certificate: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "kubernetes")
E0414 02:22:46.147409   44601 memcache.go:265] couldn't get current server API group list: Get "https://192.168.3.96:6443/api?timeout=32s": tls: failed to verify certificate: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "kubernetes")
Unable to connect to the server: tls: failed to verify certificate: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "kubernetes")
```

执行如下命令进行拷贝：

```shell
scp dys@node1:/etc/kubernetes/admin.conf /root/.kube/config
```

加入后默认角色是none，而运行apiserver等服务或者说执行init的节点是control-plane，相当于master。

```shell
dys@node1:~$ kubectl get nodes -A
NAME    STATUS   ROLES           AGE   VERSION
node1   Ready    control-plane   13h   v1.28.2
node2   Ready    <none>          13h   v1.28.2
```

执行如下命令，设置node2为worker节点。

```shell
kubectl label node node2 node-role.kubernetes.io/worker=
```

执行完成后查看节点信息：

```shell
dys@node1:~$ kubectl get nodes -A
NAME    STATUS   ROLES           AGE   VERSION
node1   Ready    control-plane   14h   v1.28.2
node2   Ready    worker          13h   v1.28.2
```

## 配置node3

node3的配置步骤和node2完全一致。

```shell
kubeadm join 192.168.3.96:6443 --token yb65a1.p045bn1algxpvl5h \
        --discovery-token-ca-cert-hash sha256:fc7bc23017475064f73df1f2d91b30a80e4a405e77519a53524109d211eb7132 --cri-socket=unix:///var/run/cri-dockerd.sock
scp dys@node1:/etc/kubernetes/admin.conf /root/.kube/config
kubectl get nodes -A
kubectl label node node3 node-role.kubernetes.io/worker=
kubectl get nodes -A
```

## 部署kubernetes-dashboard

使用官网提供的 deploy/recommended.yaml 清单安装 kubernetes-dashboard：

```shell
wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.0/aio/deploy/recommended.yaml -O  dashboard.yaml
```

执行结果如下：

```shell
root@node1:/home/dys# kubectl apply -f dashboard.yaml
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created
root@node1:/home/dys# kubectl get nodes -A
NAME    STATUS   ROLES           AGE   VERSION
node1   Ready    control-plane   15h   v1.28.2
node2   Ready    worker          15h   v1.28.2
node3   Ready    worker          68m   v1.28.2
root@node1:/home/dys# kubectl get pods -A
NAMESPACE              NAME                                         READY   STATUS             RESTARTS   AGE
kube-flannel           kube-flannel-ds-ct7kr                        1/1     Running            0          69m
kube-flannel           kube-flannel-ds-p4sqj                        1/1     Running            0          15h
kube-flannel           kube-flannel-ds-skh4v                        1/1     Running            0          15h
kube-system            coredns-66f779496c-gkdp7                     1/1     Running            0          15h
kube-system            coredns-66f779496c-rjtgp                     1/1     Running            0          15h
kube-system            etcd-node1                                   1/1     Running            0          15h
kube-system            kube-apiserver-node1                         1/1     Running            0          15h
kube-system            kube-controller-manager-node1                1/1     Running            0          15h
kube-system            kube-proxy-h2mnz                             1/1     Running            0          15h
kube-system            kube-proxy-pvw92                             1/1     Running            0          15h
kube-system            kube-proxy-xf5ff                             1/1     Running            0          69m
kube-system            kube-scheduler-node1                         1/1     Running            0          15h
kubernetes-dashboard   dashboard-metrics-scraper-6fdb9d6cdd-grwj9   0/1     ImagePullBackOff   0          5m2s
kubernetes-dashboard   kubernetes-dashboard-6fffdf99c9-fj44c        0/1     ImagePullBackOff   0          5m2s
root@node1:/home/dys#
```
查看拉取报错的原因：

```shell
kubectl -n kubernetes-dashboard describe pod kubernetes-dashboard-6fffdf99c9-fj44c
```

可以看到dashboard处于ImagePullBackOff状态，一般是拉取镜像失败（国内网络被墙的原因）。

修改dashboard.yaml中的image，将其修改为可以拉取镜像的地址（或者本地地址）,比如改成下面这样：

```json
image: docker-0.unsee.tech/kubernetesui/dashboard:v2.7.0

image: docker-0.unsee.tech/kubernetesui/metrics-scraper:v1.0.8
```

然后重新创建资源：

```shell
kubectl delete -f dashboard.yaml
kubectl apply -f dashboard.yaml
```

可以看到pod已经处于运行状态：

```shell
root@node1:/home/dys# kubectl get pod -A
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE
kube-flannel           kube-flannel-ds-ct7kr                        1/1     Running   0          3h34m
kube-flannel           kube-flannel-ds-p4sqj                        1/1     Running   0          17h
kube-flannel           kube-flannel-ds-skh4v                        1/1     Running   0          17h
kube-system            coredns-66f779496c-gkdp7                     1/1     Running   0          17h
kube-system            coredns-66f779496c-rjtgp                     1/1     Running   0          17h
kube-system            etcd-node1                                   1/1     Running   0          17h
kube-system            kube-apiserver-node1                         1/1     Running   0          17h
kube-system            kube-controller-manager-node1                1/1     Running   0          17h
kube-system            kube-proxy-h2mnz                             1/1     Running   0          17h
kube-system            kube-proxy-pvw92                             1/1     Running   0          17h
kube-system            kube-proxy-xf5ff                             1/1     Running   0          3h34m
kube-system            kube-scheduler-node1                         1/1     Running   0          17h
kubernetes-dashboard   dashboard-metrics-scraper-6d7bc7b457-njddc   1/1     Running   0          10m
kubernetes-dashboard   kubernetes-dashboard-65bbb559f-dczct         1/1     Running   0          10m
```